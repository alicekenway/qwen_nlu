# Core dependencies for Qwen2 SFT with LoRA
torch>=2.0.0
transformers>=4.40.0
datasets>=2.14.0
peft>=0.10.0
trl>=0.8.0
accelerate>=0.27.0

# Optional: for quantization (4-bit/8-bit training)
bitsandbytes>=0.41.0

# Optional: for flash attention (faster training)
flash-attn>=2.5.0

# Optional: for experiment tracking
# wandb>=0.16.0
# tensorboard>=2.15.0

